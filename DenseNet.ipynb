{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["KAGGLE_NOTEBOOK = True"]},{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7XiUOLmvIpGh","trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","from torchvision import datasets, transforms, utils\n","\n","import os\n","import random\n","from collections import defaultdict\n","from PIL import Image\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","if not KAGGLE_NOTEBOOK:\n","  !pip install opendatasets\n","\n","  import opendatasets as od\n","  import pandas\n","\n","  od.download(\"https://www.kaggle.com/datasets/mittalshubham/images256/data\")\n","\n","  !pip install wandb\n","  from google.colab import userdata\n","  wandb_api_key = userdata.get('wandb-api-key')\n","\n","  root_dir = '/content/images256'\n","\n","else:\n","  from kaggle_secrets import UserSecretsClient\n","  user_secrets = UserSecretsClient()\n","  wandb_api_key = user_secrets.get_secret(\"wandb-api-key\")\n","\n","  root_dir = '/kaggle/input'\n","\n","import wandb\n","wandb.login(key=wandb_api_key)"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["batch_size = 32\n","growth_rate = 32\n","reduction = 0.5\n","n_epochs = 500\n","learning_rate = 1e-3\n","train_size = 0.8\n","limit_per_class = 0  # 0 to disable\n","seed = 42\n","save_every = 1"]},{"cell_type":"markdown","metadata":{},"source":["# WandB"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_name = \"DenseNet\"\n","\n","wandb.init(\n","    project=\"deep-learning\",\n","    config={\n","        \"model\": model_name,\n","        \"batch_size\": batch_size,\n","        \"growth_rate\": growth_rate,\n","        \"reduction\": reduction,\n","        \"n_epochs\": n_epochs,\n","        \"learning_rate\": learning_rate,\n","        \"train_size\": train_size,\n","        \"seed\": seed,\n","        \"limit_per_class\": limit_per_class,\n","    }\n",")"]},{"cell_type":"markdown","metadata":{"id":"xZos33KvJZyI"},"source":["# PlacesDataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class PlacesDataset(data.Dataset):\n","    def __init__(self, root_dir=root_dir, train=True, train_size=train_size, seed=seed, limit_per_class=limit_per_class):\n","        super().__init__()\n","\n","        # params dataset\n","        self.root_dir = root_dir\n","        self.train = train\n","        self.limit_per_class = limit_per_class\n","\n","        # data and labels\n","        self.image_paths = []\n","        self.labels = []\n","        self.image_per_class = defaultdict(list)\n","\n","        self.number_of_classes = 0\n","        self.classes = []\n","\n","        self.transform = transforms.Compose([\n","            transforms.Resize((256, 256)),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomVerticalFlip(),\n","            transforms.RandomRotation(degrees=30), # degrees = range of rotation\n","            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # parameters are ranges\n","            transforms.RandomGrayscale(p=0.1), # p = probability of applying the transform\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","\n","        list_sub_catg = ['shop', 'outdoor', 'outdoor', 'cultivated', 'wild',\n","                         'outdoor', 'coral_reef', 'outdoor', 'indoor', 'outdoor',\n","                         'outdoor', 'indoor', 'football', 'baseball', 'platform',\n","                         'outdoor', 'outdoor', 'platform', 'south_asia', 'east_asia',\n","                         'outdoor', 'outdoor', 'home', 'outdoor', 'sand',\n","                         'vegetation']\n","\n","        class_idx = 0\n","        for letter_folder in os.listdir(root_dir):\n","            letter_folder_path = os.path.join(root_dir, letter_folder)\n","            if not os.path.isdir(letter_folder_path):\n","                continue  # Skip if not a directory\n","\n","            # Iterate through class folders within the alphabetical folder\n","            for class_name in os.listdir(letter_folder_path):\n","                class_dir = os.path.join(letter_folder_path, class_name)\n","                if not os.path.isdir(class_dir):\n","                    continue  # Skip if not a directory\n","\n","                filenames = sorted(os.listdir(class_dir))\n","                count = 0\n","                for filename in filenames:\n","                    if limit_per_class == 0 or count < limit_per_class:\n","                      # sub category is detected\n","                      if filename in list_sub_catg:\n","                        print(\n","                            f\"Sub-category detected '{filename}' and images label with parent category {class_name} {class_idx}\")\n","\n","                        sub_class_dir = os.path.join(class_dir, filename)\n","                        sub_filenames = sorted(os.listdir(sub_class_dir))\n","\n","                        for sub_filename in sub_filenames:\n","                            try:\n","                                sub_img_path = os.path.join(\n","                                    sub_class_dir, sub_filename)\n","\n","                                Image.open(sub_img_path).verify()\n","\n","                                self.image_per_class[class_idx].append(\n","                                    sub_img_path)\n","\n","                                count += 1\n","\n","                            except (IOError, SyntaxError):\n","                                print(\n","                                    'SUB Corrupted image or non-image file detected and skipped:', sub_filename)\n","                      else:\n","                        try:\n","                          img_path = os.path.join(class_dir, filename)\n","\n","                          Image.open(img_path).verify()\n","\n","                          self.image_per_class[class_idx].append(img_path)\n","\n","                          count += 1\n","\n","                        except (IOError, SyntaxError):\n","                          print(\n","                              'Corrupted image or non-image file detected and skipped:', filename)\n","                    else:\n","                        break\n","                self.number_of_classes += 1\n","                self.classes.append(class_name)\n","                class_idx += 1\n","\n","        # Lists to hold training and testing data\n","        train_img_paths = []\n","        train_labels = []\n","        test_img_paths = []\n","        test_labels = []\n","\n","        # Determining train/test split per class\n","        random.seed(seed)\n","        for label, paths in self.image_per_class.items():\n","            random.shuffle(paths)  # Shuffling within each class\n","            # Number of items in this class for train set\n","            nb_train = int(train_size * len(paths))\n","\n","            # Splitting the data for this class into train and test\n","            train_img_paths.extend(paths[:nb_train])\n","            train_labels.extend([label] * nb_train)\n","            test_img_paths.extend(paths[nb_train:])\n","            test_labels.extend([label] * (len(paths) - nb_train))\n","\n","        # Applying the split\n","        if self.train:\n","            self.image_paths = train_img_paths\n","            self.labels = train_labels\n","        else:\n","            self.image_paths = test_img_paths\n","            self.labels = test_labels\n","\n","        combined = list(zip(self.image_paths, self.labels))\n","        random.shuffle(combined)\n","        self.image_paths, self.labels = zip(*combined)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, index):\n","        image_path = self.image_paths[index]\n","        label = self.labels[index]\n","\n","        try:\n","          image = Image.open(image_path)\n","          op = 1\n","          image = image.convert('RGB')\n","          rgb = 1\n","        except (IOError, SyntaxError):\n","          print(f\"Error convert to load {op} RGB {rgb} : {image_path} {label}\")\n","\n","        image = self.transform(image)\n","\n","        return image, label"]},{"cell_type":"markdown","metadata":{},"source":["# DenseNet"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = 'cuda'\n","else :\n","    device = 'cpu'\n","\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Bottleneck(nn.Module):\n","    def __init__(self, in_channels, growth_rate):\n","        super(Bottleneck, self).__init__()\n","        inter_channels = 4 * growth_rate\n","        self.bn1 = nn.BatchNorm2d(in_channels)\n","        self.relu1 = nn.ReLU(inplace=True)\n","        self.conv1 = nn.Conv2d(in_channels, inter_channels,\n","                               kernel_size=1, bias=False)\n","        nn.init.kaiming_normal_(self.conv1.weight)\n","\n","        self.bn2 = nn.BatchNorm2d(inter_channels)\n","        self.relu2 = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(inter_channels, growth_rate,\n","                               kernel_size=3, padding=1, bias=False)\n","        nn.init.kaiming_normal_(self.conv2.weight)\n","\n","    def forward(self, x):\n","        out = self.conv1(self.relu1(self.bn1(x)))\n","        out = self.conv2(self.relu2(self.bn2(out)))\n","        out = torch.cat([x, out], 1)\n","        return out\n","\n","\n","class Transition(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(Transition, self).__init__()\n","        self.bn = nn.BatchNorm2d(in_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv = nn.Conv2d(in_channels, out_channels,\n","                              kernel_size=1, bias=False)\n","        nn.init.kaiming_normal_(self.conv.weight)\n","        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n","\n","    def forward(self, x):\n","        out = self.conv(self.relu(self.bn(x)))\n","        out = self.avg_pool(out)\n","        return out\n","\n","\n","class DenseBlock(nn.Module):\n","    def __init__(self, num_layers, in_channels, growth_rate):\n","        super(DenseBlock, self).__init__()\n","        self.layer = nn.Sequential()\n","        for i in range(num_layers):\n","            layer = Bottleneck(in_channels + i * growth_rate, growth_rate)\n","            self.layer.add_module('bottleneck_layer_{}'.format(i), layer)\n","\n","    def forward(self, x):\n","        return self.layer(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DenseNet(nn.Module):\n","    def __init__(self, growth_rate, block_layers, reduction, num_classes):\n","        super(DenseNet, self).__init__()\n","        num_features = 2 * growth_rate\n","        \n","        self.conv1 = nn.Conv2d(\n","            3, num_features, kernel_size=7, stride=2, padding=3, bias=False)\n","        nn.init.kaiming_normal_(self.conv1.weight)\n","        self.bn1 = nn.BatchNorm2d(num_features)\n","        self.relu1 = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        # Dense Blocks and Transition Layers\n","        self.dense_blocks = nn.Sequential()\n","        for i in range(len(block_layers)):\n","            # Dense Block\n","            block = DenseBlock(\n","                num_layers=block_layers[i], in_channels=num_features, growth_rate=growth_rate)\n","            self.dense_blocks.add_module('dense_block_{}'.format(i + 1), block)\n","            num_features += block_layers[i] * growth_rate\n","\n","            # Transition Layer, if not the last block\n","            if i != len(block_layers) - 1:\n","                out_features = int(num_features * reduction)\n","                trans = Transition(in_channels=num_features,\n","                                   out_channels=out_features)\n","                self.dense_blocks.add_module(\n","                    'transition_layer_{}'.format(i + 1), trans)\n","                num_features = out_features\n","\n","        # Final batch norm\n","        self.final_bn = nn.BatchNorm2d(num_features)\n","        self.final_relu = nn.ReLU(inplace=True)\n","        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(num_features, num_classes)\n","\n","    def forward(self, x):\n","        out = self.maxpool(self.relu1(self.bn1(self.conv1(x))))\n","        out = self.dense_blocks(out)\n","        out = self.final_relu(self.final_bn(out))\n","        out = self.avg_pool(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.fc(out)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"Vu505RVsJm1L"},"source":["# Train loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zs4Fk7CIpGs","trusted":true},"outputs":[],"source":["def train(model, trainloader, testloader, n_epochs=n_epochs, learning_rate=learning_rate):\n","\n","    model.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    train_avg_loss = []\n","    test_avg_loss = []\n","    test_accuracy = []\n","\n","    for i in range(n_epochs):\n","\n","        print(f\"Epoch : {i}\")\n","\n","        train_losses = []\n","        test_losses = []\n","        \n","        # train\n","        for x, y in trainloader:\n","            # send to device\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            # predict\n","            pred = model(x)\n","            loss = criterion(pred, y)\n","            train_losses.append(loss.detach())\n","\n","            # step\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        # test\n","        with torch.no_grad():\n","            correct = 0\n","\n","            for x,y in testloader:\n","                x = x.to(device)\n","                y = y.to(device)\n","\n","                pred = model(x)\n","                loss = criterion(pred, y)\n","                test_losses.append(loss.detach())\n","\n","                y_pred = pred.argmax(dim=-1)\n","                correct = correct + (y_pred==y).sum()\n","\n","            accuracy = (correct / len(testloader.dataset))\n","\n","        train_loss = torch.stack(train_losses).mean()\n","        test_loss = torch.stack(test_losses).mean()\n","\n","        print(f\"train_losses : {train_loss}\")\n","        print(f\"test_losses : {test_loss}\")\n","        print(f\"accuracy : {accuracy}\")\n","        \n","        wandb.log({\n","            \"epoch\": i,\n","            \"train loss\": train_loss,\n","            \"test loss\": test_loss,\n","            \"accuracy\": accuracy,\n","        })\n","        \n","        if i % save_every == 0:\n","            torch.save(model.state_dict(), f\"epoch_{i}_model.pt\")\n","            wandb.save(f\"epoch_{i}_model.pt\")\n","\n","        train_avg_loss.append(train_loss)\n","        test_avg_loss.append(test_loss)\n","        test_accuracy.append(accuracy)\n","\n","    return train_avg_loss, test_avg_loss, test_accuracy"]},{"cell_type":"markdown","metadata":{"id":"6bhhihxEJyKI"},"source":["# Create dataset / dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldg8clQWjX18","trusted":true},"outputs":[],"source":["# Instantiate the train and test set\n","\n","# train\n","train_dataset = PlacesDataset(train=True)\n","\n","# test\n","test_dataset = PlacesDataset(train=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htjtuyMkIpGt","trusted":true},"outputs":[],"source":["# Instantiate the corresponding data loaders\n","\n","# train\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","# test\n","test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"Lwt1N-OEJ4GM"},"source":["# Define Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0F8G1C9VIpGu","trusted":true},"outputs":[],"source":["input_features = [3, 256, 256] # Channels (assuming RGB images), Height, Width\n","output_features = train_dataset.number_of_classes\n","\n","network = DenseNet(growth_rate=growth_rate, block_layers=[\n","                           6, 12, 24, 16], reduction=reduction, num_classes=output_features)\n","print(network)"]},{"cell_type":"markdown","metadata":{"id":"4x86umi-KF2r"},"source":["# Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Qvj8tuPOIpGu","trusted":true},"outputs":[],"source":["train_avg_loss, test_avg_loss, test_accuracy = train(model=network,\n","                                                     trainloader=train_loader,\n","                                                     testloader=test_loader,\n","                                                     n_epochs=n_epochs,\n","                                                     learning_rate=learning_rate\n","                                                     )"]},{"cell_type":"markdown","metadata":{"id":"34GemFNqKO6q"},"source":["# Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fp6bD0W-IpGv","trusted":true},"outputs":[],"source":["train_avg_loss_np = torch.tensor(train_avg_loss).detach().cpu().numpy()\n","test_avg_loss_np = torch.tensor(test_avg_loss).detach().cpu().numpy()\n","test_accuracy_np = torch.tensor(test_accuracy).detach().cpu().numpy()\n","\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(train_avg_loss_np, label='Training Loss')\n","plt.plot(test_avg_loss_np, label='Testing Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Testing Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(test_accuracy_np, label='Test Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Test Accuracy')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Finish wandb run"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# necessary in notebooks\n","wandb.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":263563,"sourceId":559659,"sourceType":"datasetVersion"}],"dockerImageVersionId":30703,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
