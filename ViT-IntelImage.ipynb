{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7XiUOLmvIpGh","trusted":true},"outputs":[],"source":["import wandb\n","from kaggle_secrets import UserSecretsClient\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","from torchvision import datasets, transforms, utils\n","import os\n","import random\n","from collections import defaultdict\n","from PIL import Image\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","\n","user_secrets = UserSecretsClient()\n","wandb_api_key = user_secrets.get_secret(\"wandb-api-key\")\n","\n","root_dir = '/kaggle/input/intel-image-classification'\n","\n","wandb.login(key=wandb_api_key)"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["batch_size = 32\n","n_epochs = 500\n","learning_rate = 1e-3\n","dropout = 0.2\n","limit_per_class = 0  # 0 to disable\n","seed = 42\n","save_every = 1\n","n_patches = 32\n","n_embedding = 128"]},{"cell_type":"markdown","metadata":{},"source":["# WandB"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_name = \"ViT-IntelImage\"\n","\n","wandb.init(\n","    project=\"deep-learning\",\n","    config={\n","        \"model\": model_name,\n","        \"batch_size\": batch_size,\n","        \"n_epochs\": n_epochs,\n","        \"n_patches\": n_patches,\n","        \"n_embedding\": n_embedding,\n","        \"learning_rate\": learning_rate,\n","        \"dropout\": dropout,\n","        \"seed\": seed,\n","        \"limit_per_class\": limit_per_class,\n","    }\n",")"]},{"cell_type":"markdown","metadata":{"id":"xZos33KvJZyI"},"source":["# IntelImageDataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class IntelImageDataset(data.Dataset):\n","    def __init__(self, root_dir=root_dir, train=True, seed=seed, limit_per_class=limit_per_class):\n","        super().__init__()\n","\n","        # params dataset\n","        self.root_dir = root_dir\n","        self.train = train\n","        self.limit_per_class = limit_per_class\n","\n","        # data and labels\n","        self.image_paths = []\n","        self.labels = []\n","\n","        self.number_of_classes = 0\n","        self.classes = []\n","\n","        self.transform = transforms.Compose([\n","            transforms.Resize((256, 256)),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomVerticalFlip(),\n","            transforms.RandomRotation(degrees=30), # degrees = range of rotation\n","            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # parameters are ranges\n","            transforms.RandomGrayscale(p=0.1), # p = probability of applying the transform\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","\n","        data_dir = os.path.join(root_dir, 'seg_train/seg_train' if train else 'seg_test/seg_test')\n","\n","        # Iterate through class folders within the alphabetical folder\n","        for class_name in os.listdir(data_dir):\n","            class_dir = os.path.join(data_dir, class_name)\n","            if not os.path.isdir(class_dir):\n","                continue  # Skip if not a directory\n","\n","            filenames = sorted(os.listdir(class_dir))\n","            count = 0\n","            for filename in filenames:\n","                if limit_per_class == 0 or count < limit_per_class:\n","                    try:\n","                      img_path = os.path.join(class_dir, filename)\n","\n","                      Image.open(img_path).verify()\n","\n","                      self.image_paths.append(img_path)\n","                      self.labels.append(self.number_of_classes)\n","\n","                      count += 1\n","\n","                    except (IOError, SyntaxError):\n","                      print(\n","                          'Corrupted image or non-image file detected and skipped:', filename)\n","                else:\n","                    break\n","                    \n","            self.number_of_classes += 1\n","            self.classes.append(class_name)\n","\n","        random.seed(seed)\n","        combined = list(zip(self.image_paths, self.labels))\n","        random.shuffle(combined)\n","        self.image_paths, self.labels = zip(*combined)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, index):\n","        image_path = self.image_paths[index]\n","        label = self.labels[index]\n","\n","        try:\n","          image = Image.open(image_path)\n","          op = 1\n","          image = image.convert('RGB')\n","          rgb = 1\n","        except (IOError, SyntaxError):\n","          print(f\"Error convert to load {op} RGB {rgb} : {image_path} {label}\")\n","\n","        image = self.transform(image)\n","\n","        return image, label"]},{"cell_type":"markdown","metadata":{"id":"rW6Ju2wtJjQh"},"source":["# ViT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djE8-SIgIpGq","trusted":true},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = 'cuda'\n","else :\n","    device = 'cpu'\n","\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DfCk4fZMIpGr","trusted":true},"outputs":[],"source":["def patchify(images, n_patches):\n","    n, c, h, w = images.shape\n","\n","    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n","    patch_size = h // n_patches\n","\n","    for idx, image in enumerate(images):\n","        for i in range(n_patches):\n","            for j in range(n_patches):\n","                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n","                patches[idx, i * n_patches + j] = patch.flatten()\n","    return patches"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, n_heads, head_size, n_patches, n_embedding):\n","        super(MultiHeadAttention, self).__init__()\n","        self.n_heads = n_heads\n","        self.head_size = head_size\n","        \n","        self.q_mappings = nn.ModuleList([nn.Linear(self.head_size, self.head_size) for _ in range(self.n_heads)])\n","        self.k_mappings = nn.ModuleList([nn.Linear(self.head_size, self.head_size) for _ in range(self.n_heads)])\n","        self.v_mappings = nn.ModuleList([nn.Linear(self.head_size, self.head_size) for _ in range(self.n_heads)])\n","        self.head_dropout = nn.ModuleList([nn.Dropout(dropout) for _ in range(self.n_heads)])\n","\n","        self.softmax = nn.Softmax(dim=-1)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, sequences):\n","        # B : batch size\n","        # L : length of the sequence\n","        # D : dimension of each token\n","        # Sequences has shape (B, L, D)\n","        \n","        # N : number of heads\n","        # S : size of each head \n","        \n","        result = []\n","        for sequence in sequences:\n","            seq_result = []\n","            for head in range(self.n_heads):\n","                q_mapping = self.q_mappings[head]\n","                k_mapping = self.k_mappings[head]\n","                v_mapping = self.v_mappings[head]\n","                \n","                #each head operates on a given segment of the sequence\n","                seq = sequence[:, head * self.head_size: (head + 1) * self.head_size] #slices of shape (L, D_h)\n","                \n","                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq) #slices of shape (L, D_h)\n","\n","                attention = self.softmax(q @ k.T / (self.head_size ** 0.5)) # (L, D_h) x (D_h, L) --> (L, L)\n","                dropout_layer = self.head_dropout[head]\n","                attention = dropout_layer(attention)  #(L, L)\n","                \n","                seq_result.append(attention @ v) #(L, L) x (L, D_h) --> (L, D_h)\n","                \n","            #concatenate \n","            result.append(torch.hstack(seq_result)) # concatenation of N slice of shape (L, D_h) --> (L, D)\n","            \n","        h = torch.cat([torch.unsqueeze(r, dim=0) for r in result])#stack all sequences along a new dimesions : back to (B, L, D)\n","        return h"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class FeedForward(nn.Module):\n","    def __init__(self, n_embedding):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embedding, 4*n_embedding),\n","            nn.GELU(),\n","            nn.Linear(4*n_embedding, n_embedding),\n","            nn.Dropout(dropout)\n","        )\n","    \n","    def forward(self, x):\n","        return self.net(x)  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Block(nn.Module):\n","    def __init__(self, n_embedding, n_heads, n_patches):\n","        super(Block, self).__init__()\n","        self.n_embedding = n_embedding\n","        self.n_heads = n_heads\n","        self.n_patches = n_patches\n","\n","        head_size = self.n_embedding // n_heads\n","\n","        self.sa = MultiHeadAttention(self.n_heads, head_size, self.n_patches, self.n_embedding)\n","        self.ff = FeedForward(self.n_embedding)\n","        self.ln1 = nn.LayerNorm(self.n_embedding)\n","        self.ln2 = nn.LayerNorm(self.n_embedding)\n","\n","\n","    def forward(self, x):\n","        h = x\n","        h = h + self.sa(self.ln1(h))\n","        h = h + self.ff(self.ln2(h))\n","        return h"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class VisualTransformer(nn.Module):\n","    def __init__(self, dimensions, n_patches, n_embedding, n_heads, output_size):\n","        # Super constructor\n","        super(VisualTransformer, self).__init__()\n","        \n","        # Attributes\n","        self.dimensions = dimensions \n","        self.n_patches = n_patches\n","        self.n_heads = n_heads\n","        self.n_embedding = n_embedding\n","        self.output_size = output_size\n","        \n","        self.patch_size = (dimensions[1] / n_patches, dimensions[2] / n_patches)\n","\n","        # 1) Linear mapper\n","        self.input_d = int(dimensions[0] * self.patch_size[0] * self.patch_size[1])\n","        self.linear_mapper = nn.Linear(self.input_d, self.n_embedding)\n","        \n","        # 2) Learnable classification token\n","        self.class_token = nn.Parameter(torch.rand(1, self.n_embedding))\n","        \n","        # 3) Positional embedding\n","        self.positional_embeddings = nn.Embedding(self.n_patches**2 + 1, self.n_embedding)\n","        \n","        # 4) Transformer encoder blocks\n","        self.blocks = nn.Sequential(\n","            Block(self.n_embedding, self.n_heads, self.n_patches),\n","            Block(self.n_embedding, self.n_heads, self.n_patches),\n","            Block(self.n_embedding, self.n_heads, self.n_patches),\n","            nn.LayerNorm(self.n_embedding)\n","        )\n","        \n","        # 5) Classification MLP\n","        self.lm_head = nn.Sequential(\n","            nn.Linear(self.n_embedding, self.output_size),\n","            nn.Softmax(dim=-1)\n","        )\n","\n","    def forward(self, images):\n","        # Patchify\n","        patches = patchify(images, self.n_patches).to(device)\n","        \n","        #Token Embedding\n","        tok_emb = self.linear_mapper(patches)\n","        \n","        # Adding classification token\n","        tok_emb = torch.stack([torch.vstack((self.class_token, tok_emb[i]))for i in range(len(tok_emb))])\n","        \n","        #Positional Embedding\n","        pos_emb = self.positional_embeddings(torch.arange(self.n_patches**2 + 1, device=device))\n","        pos_emb = pos_emb.unsqueeze(0)\n","        pos_emb = pos_emb.expand(tok_emb.shape[0], -1, -1)  # Expand along the batch dimension\n","        \n","        #Sum Token and Positional Embedding\n","        h = tok_emb + pos_emb\n","        \n","        # Transformer Blocks\n","        h = self.blocks(h)\n","            \n","        # Getting the classification token only\n","        h = h[:, 0]\n","        \n","        #Final head\n","        h = self.lm_head(h)\n","        \n","        return h"]},{"cell_type":"markdown","metadata":{"id":"Vu505RVsJm1L"},"source":["# Train loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zs4Fk7CIpGs","trusted":true},"outputs":[],"source":["def train(model, trainloader, testloader, n_epochs=n_epochs, learning_rate=learning_rate):\n","\n","    model.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    train_avg_loss = []\n","    test_avg_loss = []\n","    test_accuracy = []\n","\n","    for i in range(n_epochs):\n","\n","        print(f\"Epoch : {i}\")\n","\n","        train_losses = []\n","        test_losses = []\n","        \n","        # train\n","        for x, y in trainloader:\n","            # send to device\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            # predict\n","            pred = model(x)\n","            loss = criterion(pred, y)\n","            train_losses.append(loss.detach())\n","\n","            # step\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        # test\n","        with torch.no_grad():\n","            correct = 0\n","\n","            for x,y in testloader:\n","                x = x.to(device)\n","                y = y.to(device)\n","\n","                pred = model(x)\n","                loss = criterion(pred, y)\n","                test_losses.append(loss.detach())\n","\n","                y_pred = pred.argmax(dim=-1)\n","                correct = correct + (y_pred==y).sum()\n","\n","            accuracy = (correct / len(testloader.dataset))\n","\n","        train_loss = torch.stack(train_losses).mean()\n","        test_loss = torch.stack(test_losses).mean()\n","\n","        print(f\"train_losses : {train_loss}\")\n","        print(f\"test_losses : {test_loss}\")\n","        print(f\"accuracy : {accuracy}\")\n","        \n","        wandb.log({\n","            \"epoch\": i,\n","            \"train loss\": train_loss,\n","            \"test loss\": test_loss,\n","            \"accuracy\": accuracy,\n","        })\n","        \n","        if i % save_every == 0:\n","            torch.save(model.state_dict(), f\"epoch_{i}_model.pt\")\n","            wandb.save(f\"epoch_{i}_model.pt\")\n","\n","        train_avg_loss.append(train_loss)\n","        test_avg_loss.append(test_loss)\n","        test_accuracy.append(accuracy)\n","\n","    return train_avg_loss, test_avg_loss, test_accuracy"]},{"cell_type":"markdown","metadata":{"id":"6bhhihxEJyKI"},"source":["# Create dataset / dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldg8clQWjX18","trusted":true},"outputs":[],"source":["# Instantiate the train and test set\n","\n","# train\n","train_dataset = IntelImageDataset(train=True)\n","\n","# test\n","test_dataset = IntelImageDataset(train=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htjtuyMkIpGt","trusted":true},"outputs":[],"source":["# Instantiate the corresponding data loaders\n","\n","# train\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","# test\n","test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"Lwt1N-OEJ4GM"},"source":["# Define Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0F8G1C9VIpGu","trusted":true},"outputs":[],"source":["input_features = [3, 256, 256] # Channels (assuming RGB images), Height, Width\n","output_features = train_dataset.number_of_classes\n","\n","network = VisualTransformer(input_features, n_patches, n_embedding, 4, output_features).to(device)\n","print(\"Parameters:\", sum(p.numel() for p in network.parameters())/1e3, 'K parameters')\n","print(network)"]},{"cell_type":"markdown","metadata":{"id":"4x86umi-KF2r"},"source":["# Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Qvj8tuPOIpGu","trusted":true},"outputs":[],"source":["train_avg_loss, test_avg_loss, test_accuracy = train(model=network,\n","                                                     trainloader=train_loader,\n","                                                     testloader=test_loader,\n","                                                     n_epochs=n_epochs,\n","                                                     learning_rate=learning_rate\n","                                                     )"]},{"cell_type":"markdown","metadata":{"id":"34GemFNqKO6q"},"source":["# Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fp6bD0W-IpGv","trusted":true},"outputs":[],"source":["train_avg_loss_np = torch.tensor(train_avg_loss).detach().cpu().numpy()\n","test_avg_loss_np = torch.tensor(test_avg_loss).detach().cpu().numpy()\n","test_accuracy_np = torch.tensor(test_accuracy).detach().cpu().numpy()\n","\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(train_avg_loss_np, label='Training Loss')\n","plt.plot(test_avg_loss_np, label='Testing Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Testing Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(test_accuracy_np, label='Test Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Test Accuracy')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Finish wandb run"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# necessary in notebooks\n","wandb.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":263563,"sourceId":559659,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
