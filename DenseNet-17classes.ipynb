{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["KAGGLE_NOTEBOOK = True"]},{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7XiUOLmvIpGh","trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","from torchvision import datasets, transforms, utils\n","\n","import os\n","import random\n","from collections import defaultdict\n","from PIL import Image\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","if not KAGGLE_NOTEBOOK:\n","  !pip install opendatasets\n","\n","  import opendatasets as od\n","  import pandas\n","\n","  od.download(\"https://www.kaggle.com/datasets/mittalshubham/images256/data\")\n","\n","  !pip install wandb\n","  from google.colab import userdata\n","  wandb_api_key = userdata.get('wandb-api-key')\n","\n","  root_dir = '/content/images256'\n","\n","else:\n","  from kaggle_secrets import UserSecretsClient\n","  user_secrets = UserSecretsClient()\n","  wandb_api_key = user_secrets.get_secret(\"wandb-api-key\")\n","\n","  root_dir = '/kaggle/input'\n","\n","import wandb\n","wandb.login(key=wandb_api_key)"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["batch_size = 32\n","growth_rate = 32\n","reduction = 0.5\n","n_epochs = 500\n","learning_rate = 1e-3\n","train_size = 0.8\n","limit_per_class = 0  # 0 to disable\n","seed = 42\n","save_every = 1"]},{"cell_type":"markdown","metadata":{},"source":["# WandB"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_name = \"DenseNet-17classes\"\n","\n","wandb.init(\n","    project=\"deep-learning\",\n","    config={\n","        \"model\": model_name,\n","        \"batch_size\": batch_size,\n","        \"growth_rate\": growth_rate,\n","        \"reduction\": reduction,\n","        \"n_epochs\": n_epochs,\n","        \"learning_rate\": learning_rate,\n","        \"train_size\": train_size,\n","        \"seed\": seed,\n","        \"limit_per_class\": limit_per_class,\n","    }\n",")"]},{"cell_type":"markdown","metadata":{"id":"xZos33KvJZyI"},"source":["# PlacesDataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class PlacesDataset(data.Dataset):\n","    def __init__(self, root_dir=root_dir, train=True, train_size=train_size, seed=seed, limit_per_class=limit_per_class):\n","        super().__init__()\n","\n","        # params dataset\n","        self.root_dir = root_dir\n","        self.train = train\n","        self.limit_per_class = limit_per_class\n","\n","        # data and labels\n","        self.image_paths = []\n","        self.labels = []\n","\n","        self.number_of_classes = 0\n","        self.classes = []\n","        self.old_labels = []\n","\n","        self.transform = transforms.Compose([\n","            transforms.Resize((256, 256)),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomVerticalFlip(),\n","            transforms.RandomRotation(degrees=30), # degrees = range of rotation\n","            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # parameters are ranges\n","            transforms.RandomGrayscale(p=0.1), # p = probability of applying the transform\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","\n","        list_sub_catg = ['shop', 'outdoor', 'outdoor', 'cultivated', 'wild',\n","                         'outdoor', 'coral_reef', 'outdoor', 'indoor', 'outdoor',\n","                         'outdoor', 'indoor', 'football', 'baseball', 'platform',\n","                         'outdoor', 'outdoor', 'platform', 'south_asia', 'east_asia',\n","                         'outdoor', 'outdoor', 'home', 'outdoor', 'sand',\n","                         'vegetation']\n","\n","        self.class_mapping = {\n","            # SKIP\n","            'ballroom': 'skip', 'courtyard': 'skip', 'fire_escape': 'skip', 'garbage_dump': 'skip', 'hospital': 'skip', 'hot_spring': 'skip', 'inn': 'skip',\n","            'phone_booth': 'skip', 'picnic_area': 'skip', 'reception': 'skip', 'restaurant_kitchen': 'skip', 'sky': 'skip', 'tree_farm': 'skip', 'trench': 'skip',\n","            'valley': 'skip', 'water_tower': 'skip', 'wind_farm': 'skip', 'windmill': 'skip',\n","\n","            # highly frequented public\n","            'airport_terminal': 'high_freq_pub', 'amusement_park': 'high_freq_pub', 'bar': 'high_freq_pub', 'food_court': 'high_freq_pub', 'cafeteria': 'high_freq_pub', 'playground': 'high_freq_pub', 'plaza': 'high_freq_pub',\n","            'pulpit': 'high_freq_pub', 'restaurant': 'high_freq_pub', 'restaurant_patio': 'high_freq_pub', 'stage': 'high_freq_pub', 'subway_station': 'high_freq_pub', 'train_station': 'high_freq_pub', 'platform': 'high_freq_pub',\n","\n","            # highly frequented not public\n","            'auditorium': 'high_freq_n_pub', 'banquet_hall': 'high_freq_n_pub', 'conference_center': 'high_freq_n_pub', 'classroom': 'high_freq_n_pub', 'kindergarden_classroom': 'high_freq_n_pub', 'lobby': 'high_freq_n_pub',\n","            'locker_room': 'high_freq_n_pub', 'office': 'high_freq_n_pub', 'television_studio': 'high_freq_n_pub',\n","\n","            # monument\n","            'basilica': 'monument', 'bridge': 'monument', 'castle': 'monument', 'cathedral': 'monument', 'abbey': 'monument', 'amphitheater': 'monument', 'aqueduct': 'monument', 'arch': 'monument',\n","            'cemetery': 'monument', 'church': 'monument', 'dam': 'monument', 'kasbah': 'monument', 'lighthouse': 'monument', 'mausoleum': 'monument', 'monastery': 'monument', 'pagoda': 'monument',\n","            'palace': 'monument', 'pavilion': 'monument', 'ruin': 'monument', 'temple': 'monument', 'tower': 'monument', 'viaduct': 'monument',\n","\n","            # urban\n","            'apartment_building': 'urban', 'building_facade': 'urban', 'gas_station': 'urban', 'medina': 'urban', 'office_building': 'urban', 'skyscraper': 'urban',\n","            'slum': 'urban',\n","\n","            # aquatic\n","            'aquarium': 'aquatic', 'ocean': 'aquatic', 'raft': 'aquatic', 'river': 'aquatic', 'underwater': 'aquatic',\n","\n","            # near water\n","            'bayou': 'near_water', 'coast': 'near_water', 'creek': 'near_water', 'dock': 'near_water', 'fountain': 'near_water', 'harbor': 'near_water', 'iceberg': 'near_water', 'islet': 'near_water', 'marsh': 'near_water',\n","            'pond': 'near_water', 'sandbar': 'near_water', 'swamp': 'near_water', 'swimming_pool': 'near_water', 'watering_hole': 'near_water',\n","\n","            # commercial\n","            'art_gallery': 'commercial', 'art_studio': 'commercial', 'bakery': 'commercial', 'beauty_salon': 'commercial', 'bookstore': 'commercial', 'bowling_alley': 'commercial',\n","            'butchers_shop': 'commercial', 'candy_store': 'commercial', 'clothing_store': 'commercial', 'coffee_shop': 'commercial', 'gift_shop': 'commercial',\n","            'ice_cream_parlor': 'commercial', 'laundromat': 'commercial', 'market': 'commercial', 'museum': 'commercial', 'shoe_shop': 'commercial', 'shopfront': 'commercial', 'supermarket': 'commercial',\n","\n","            # industrial\n","            'assembly_line': 'industrial', 'construction_site': 'industrial', 'engine_room': 'industrial', 'excavation': 'industrial', 'fire_station': 'industrial',\n","\n","            # domestic\n","            'attic': 'domestic', 'basement': 'domestic', 'bedroom': 'domestic', 'closet': 'domestic', 'conference_room': 'domestic', 'corridor': 'domestic', 'dinette': 'domestic',\n","            'dining_room': 'domestic', 'dorm_room': 'domestic', 'galley': 'domestic', 'game_room': 'domestic', 'home_office': 'domestic', 'hospital_room': 'domestic', 'hotel_room': 'domestic',\n","            'jail_cell': 'domestic', 'kitchen': 'domestic', 'kitchenette': 'domestic', 'living_room': 'domestic', 'music_studio': 'domestic', 'nursery': 'domestic', 'pantry': 'domestic',\n","            'parlor': 'domestic', 'shower': 'domestic', 'staircase': 'domestic', 'veranda': 'domestic', 'waiting_room': 'domestic',\n","\n","            # natural vegetal\n","            'bamboo_forest': 'nat_vege', 'campsite': 'nat_vege', 'forest_path': 'nat_vege', 'rainforest': 'nat_vege', 'rope_bridge': 'nat_vege',\n","\n","            # natural mineral\n","            'badlands': 'nat_miner', 'butte': 'nat_miner', 'canyon': 'nat_miner', 'crevasse': 'nat_miner', 'desert': 'nat_miner', 'mountain': 'nat_miner', 'mountain_snowy': 'nat_miner',\n","            'rock_arch': 'nat_miner', 'sea_cliff': 'nat_miner', 'snowfield': 'nat_miner', 'volcano': 'nat_miner',\n","\n","            # domesticated nature\n","            'botanical_garden': 'domest_nat', 'corn_field': 'domest_nat', 'cottage_garden': 'domest_nat', 'fairway': 'domest_nat', 'field': 'domest_nat', 'formal_garden': 'domest_nat',\n","            'golf_course': 'domest_nat', 'herb_garden': 'domest_nat', 'orchard': 'domest_nat', 'pasture': 'domest_nat', 'rice_paddy': 'domest_nat', 'shed': 'domest_nat',\n","            'topiary_garden': 'domest_nat', 'vegetable_garden': 'domest_nat', 'wheat_field': 'domest_nat', 'yard': 'domest_nat',\n","\n","            # sport\n","            'baseball_field': 'sport', 'boxing_ring': 'sport', 'ice_skating_rink': 'sport', 'martial_arts_gym': 'sport', 'racecourse': 'sport',\n","            'ski_resort': 'sport', 'ski_slope': 'sport', 'stadium': 'sport', 'track': 'sport',\n","\n","            # transport\n","            'boat_deck': 'transport', 'bus_interior': 'transport', 'cockpit': 'transport',\n","\n","            # homes\n","            'chalet': 'home', 'courthouse': 'home', 'doorway': 'home', 'driveway': 'home', 'hotel': 'home', 'igloo': 'home', 'mansion': 'home', 'motel': 'home',\n","            'patio': 'home', 'residential_neighborhood': 'home', 'schoolhouse': 'home',\n","\n","            # roads\n","            'forest_road': 'roads', 'alley': 'roads', 'boardwalk': 'roads', 'crosswalk': 'roads', 'highway': 'roads', 'parking_lot': 'roads',\n","            'railroad_track': 'roads', 'runway': 'roads', 'train_railway': 'roads'\n","        }\n","\n","        keys = ['domest_nat', 'home', 'monument', 'transport', 'near_water', 'high_freq_n_pub', 'aquatic', 'commercial',\n","                'industrial', 'domestic', 'high_freq_pub', 'nat_vege', 'sport', 'nat_miner', 'roads', 'urban', 'skip']\n","        values = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n","\n","        self.nb_mapping = {k: v for k, v in zip(keys, values)}\n","\n","        self.number_of_classes = len(keys) - 1\n","\n","        class_idx = 0\n","        for letter_folder in os.listdir(root_dir):\n","            letter_folder_path = os.path.join(root_dir, letter_folder)\n","            if not os.path.isdir(letter_folder_path):\n","                continue  # Skip if not a directory\n","\n","            # Iterate through class folders within the alphabetical folder\n","            for class_name in os.listdir(letter_folder_path):\n","                class_dir = os.path.join(letter_folder_path, class_name)\n","                if not os.path.isdir(class_dir):\n","                    continue  # Skip if not a directory\n","\n","                filenames = sorted(os.listdir(class_dir))\n","                count = 0\n","                for filename in filenames:\n","                    if limit_per_class == 0 or count < limit_per_class:\n","                      # sub category is detected\n","                      if filename in list_sub_catg:\n","                        print(\n","                            f\"Sub-category detected '{filename}' and images label with parent category {class_name} {class_idx}\")\n","\n","                        sub_class_dir = os.path.join(class_dir, filename)\n","                        sub_filenames = sorted(os.listdir(sub_class_dir))\n","\n","                        for sub_filename in sub_filenames:\n","                          if limit_per_class == 0 or count < limit_per_class:\n","                              try:\n","\n","                                  label_map = self.class_mapping[class_name]\n","                                  if label_map != 'skip':\n","                                    sub_im_path = os.path.join(\n","                                        sub_class_dir, sub_filename)\n","\n","                                    Image.open(sub_im_path).verify()\n","\n","                                    self.image_paths.append(sub_im_path)\n","\n","                                    self.old_labels.append(class_name)\n","\n","                                    label_idx = self.nb_mapping[label_map]\n","\n","                                    self.labels.append(label_idx)\n","\n","                                    count += 1\n","                                  else:\n","                                    print(f\"skip class {class_name}\")\n","                                    break\n","\n","                              except (IOError, SyntaxError):\n","                                  print(\n","                                      'SUB Corrupted image or non-image file detected and skipped:', sub_filename)\n","                      else:\n","                        try:\n","                            label_map = self.class_mapping[class_name]\n","                            if label_map != 'skip':\n","                                im_path = os.path.join(class_dir, filename)\n","\n","                                Image.open(im_path).verify()\n","\n","                                self.image_paths.append(im_path)\n","\n","                                self.old_labels.append(class_name)\n","\n","                                label_map = self.class_mapping[class_name]\n","\n","                                label_idx = self.nb_mapping[label_map]\n","\n","                                self.labels.append(label_idx)\n","\n","                                count += 1\n","                            else:\n","                                print(f\"skip class {class_name}\")\n","                                break\n","\n","                        except (IOError, SyntaxError):\n","                          print(\n","                              'Corrupted image or non-image file detected and skipped:', filename)\n","                    else:\n","                        break\n","                self.classes.append(class_name)\n","                class_idx += 1\n","\n","        combined = list(zip(self.image_paths, self.labels))\n","\n","        random.seed(seed)\n","        random.shuffle(combined)\n","\n","        self.image_paths, self.labels = zip(*combined)\n","\n","        nb_train = int(train_size * len(self.image_paths))\n","\n","        # Dividing the dataset\n","        if self.train:\n","          self.image_paths = self.image_paths[:nb_train]\n","          self.labels = self.labels[:nb_train]\n","        else:\n","          self.image_paths = self.image_paths[nb_train:]\n","          self.labels = self.labels[nb_train:]\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, index):\n","        image_path = self.image_paths[index]\n","        label = self.labels[index]\n","\n","        try:\n","          image = Image.open(image_path)\n","          op = 1\n","          image = image.convert('RGB')\n","          rgb = 1\n","        except (IOError, SyntaxError):\n","          print(f\"Error convert to load {op} RGB {rgb} : {image_path} {label}\")\n","\n","        image = self.transform(image)\n","\n","        return image, label"]},{"cell_type":"markdown","metadata":{},"source":["# DenseNet"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = 'cuda'\n","else :\n","    device = 'cpu'\n","\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Bottleneck(nn.Module):\n","    def __init__(self, in_channels, growth_rate):\n","        super(Bottleneck, self).__init__()\n","        inter_channels = 4 * growth_rate\n","        self.bn1 = nn.BatchNorm2d(in_channels)\n","        self.relu1 = nn.ReLU(inplace=True)\n","        self.conv1 = nn.Conv2d(in_channels, inter_channels,\n","                               kernel_size=1, bias=False)\n","        nn.init.kaiming_normal_(self.conv1.weight)\n","\n","        self.bn2 = nn.BatchNorm2d(inter_channels)\n","        self.relu2 = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(inter_channels, growth_rate,\n","                               kernel_size=3, padding=1, bias=False)\n","        nn.init.kaiming_normal_(self.conv2.weight)\n","\n","    def forward(self, x):\n","        out = self.conv1(self.relu1(self.bn1(x)))\n","        out = self.conv2(self.relu2(self.bn2(out)))\n","        out = torch.cat([x, out], 1)\n","        return out\n","\n","\n","class Transition(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(Transition, self).__init__()\n","        self.bn = nn.BatchNorm2d(in_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv = nn.Conv2d(in_channels, out_channels,\n","                              kernel_size=1, bias=False)\n","        nn.init.kaiming_normal_(self.conv.weight)\n","        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n","\n","    def forward(self, x):\n","        out = self.conv(self.relu(self.bn(x)))\n","        out = self.avg_pool(out)\n","        return out\n","\n","\n","class DenseBlock(nn.Module):\n","    def __init__(self, num_layers, in_channels, growth_rate):\n","        super(DenseBlock, self).__init__()\n","        self.layer = nn.Sequential()\n","        for i in range(num_layers):\n","            layer = Bottleneck(in_channels + i * growth_rate, growth_rate)\n","            self.layer.add_module('bottleneck_layer_{}'.format(i), layer)\n","\n","    def forward(self, x):\n","        return self.layer(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DenseNet(nn.Module):\n","    def __init__(self, growth_rate, block_layers, reduction, num_classes):\n","        super(DenseNet, self).__init__()\n","        num_features = 2 * growth_rate\n","        \n","        self.conv1 = nn.Conv2d(\n","            3, num_features, kernel_size=7, stride=2, padding=3, bias=False)\n","        nn.init.kaiming_normal_(self.conv1.weight)\n","        self.bn1 = nn.BatchNorm2d(num_features)\n","        self.relu1 = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        # Dense Blocks and Transition Layers\n","        self.dense_blocks = nn.Sequential()\n","        for i in range(len(block_layers)):\n","            # Dense Block\n","            block = DenseBlock(\n","                num_layers=block_layers[i], in_channels=num_features, growth_rate=growth_rate)\n","            self.dense_blocks.add_module('dense_block_{}'.format(i + 1), block)\n","            num_features += block_layers[i] * growth_rate\n","\n","            # Transition Layer, if not the last block\n","            if i != len(block_layers) - 1:\n","                out_features = int(num_features * reduction)\n","                trans = Transition(in_channels=num_features,\n","                                   out_channels=out_features)\n","                self.dense_blocks.add_module(\n","                    'transition_layer_{}'.format(i + 1), trans)\n","                num_features = out_features\n","\n","        # Final batch norm\n","        self.final_bn = nn.BatchNorm2d(num_features)\n","        self.final_relu = nn.ReLU(inplace=True)\n","        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(num_features, num_classes)\n","\n","    def forward(self, x):\n","        out = self.maxpool(self.relu1(self.bn1(self.conv1(x))))\n","        out = self.dense_blocks(out)\n","        out = self.final_relu(self.final_bn(out))\n","        out = self.avg_pool(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.fc(out)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"Vu505RVsJm1L"},"source":["# Train loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zs4Fk7CIpGs","trusted":true},"outputs":[],"source":["def train(model, trainloader, testloader, n_epochs=n_epochs, learning_rate=learning_rate):\n","\n","    model.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    train_avg_loss = []\n","    test_avg_loss = []\n","    test_accuracy = []\n","\n","    for i in range(n_epochs):\n","\n","        print(f\"Epoch : {i}\")\n","\n","        train_losses = []\n","        test_losses = []\n","        \n","        # train\n","        for x, y in trainloader:\n","            # send to device\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            # predict\n","            pred = model(x)\n","            loss = criterion(pred, y)\n","            train_losses.append(loss.detach())\n","\n","            # step\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        # test\n","        with torch.no_grad():\n","            correct = 0\n","\n","            for x,y in testloader:\n","                x = x.to(device)\n","                y = y.to(device)\n","\n","                pred = model(x)\n","                loss = criterion(pred, y)\n","                test_losses.append(loss.detach())\n","\n","                y_pred = pred.argmax(dim=-1)\n","                correct = correct + (y_pred==y).sum()\n","\n","            accuracy = (correct / len(testloader.dataset))\n","\n","        train_loss = torch.stack(train_losses).mean()\n","        test_loss = torch.stack(test_losses).mean()\n","\n","        print(f\"train_losses : {train_loss}\")\n","        print(f\"test_losses : {test_loss}\")\n","        print(f\"accuracy : {accuracy}\")\n","        \n","        wandb.log({\n","            \"epoch\": i,\n","            \"train loss\": train_loss,\n","            \"test loss\": test_loss,\n","            \"accuracy\": accuracy,\n","        })\n","        \n","        if i % save_every == 0:\n","            torch.save(model.state_dict(), f\"epoch_{i}_model.pt\")\n","            wandb.save(f\"epoch_{i}_model.pt\")\n","\n","        train_avg_loss.append(train_loss)\n","        test_avg_loss.append(test_loss)\n","        test_accuracy.append(accuracy)\n","\n","    return train_avg_loss, test_avg_loss, test_accuracy"]},{"cell_type":"markdown","metadata":{"id":"6bhhihxEJyKI"},"source":["# Create dataset / dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldg8clQWjX18","trusted":true},"outputs":[],"source":["# Instantiate the train and test set\n","\n","# train\n","train_dataset = PlacesDataset(train=True)\n","\n","# test\n","test_dataset = PlacesDataset(train=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htjtuyMkIpGt","trusted":true},"outputs":[],"source":["# Instantiate the corresponding data loaders\n","\n","# train\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","# test\n","test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"Lwt1N-OEJ4GM"},"source":["# Define Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0F8G1C9VIpGu","trusted":true},"outputs":[],"source":["input_features = [3, 256, 256] # Channels (assuming RGB images), Height, Width\n","output_features = train_dataset.number_of_classes\n","\n","network = DenseNet(growth_rate=growth_rate, block_layers=[\n","                           6, 12, 24, 16], reduction=reduction, num_classes=output_features)\n","print(network)"]},{"cell_type":"markdown","metadata":{"id":"4x86umi-KF2r"},"source":["# Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Qvj8tuPOIpGu","trusted":true},"outputs":[],"source":["train_avg_loss, test_avg_loss, test_accuracy = train(model=network,\n","                                                     trainloader=train_loader,\n","                                                     testloader=test_loader,\n","                                                     n_epochs=n_epochs,\n","                                                     learning_rate=learning_rate\n","                                                     )"]},{"cell_type":"markdown","metadata":{"id":"34GemFNqKO6q"},"source":["# Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fp6bD0W-IpGv","trusted":true},"outputs":[],"source":["train_avg_loss_np = torch.tensor(train_avg_loss).detach().cpu().numpy()\n","test_avg_loss_np = torch.tensor(test_avg_loss).detach().cpu().numpy()\n","test_accuracy_np = torch.tensor(test_accuracy).detach().cpu().numpy()\n","\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(train_avg_loss_np, label='Training Loss')\n","plt.plot(test_avg_loss_np, label='Testing Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Testing Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(test_accuracy_np, label='Test Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Test Accuracy')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Finish wandb run"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# necessary in notebooks\n","wandb.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":263563,"sourceId":559659,"sourceType":"datasetVersion"}],"dockerImageVersionId":30703,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
