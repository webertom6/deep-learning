{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["KAGGLE_NOTEBOOK = True"]},{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7XiUOLmvIpGh","trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","from torchvision import datasets, transforms, utils\n","import os\n","import random\n","from collections import defaultdict\n","from PIL import Image\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","if not KAGGLE_NOTEBOOK:\n","  !pip install opendatasets\n","\n","  import opendatasets as od\n","  import pandas\n","\n","  od.download(\"https://www.kaggle.com/datasets/mittalshubham/images256/data\")\n","\n","  !pip install wandb\n","  from google.colab import userdata\n","  wandb_api_key = userdata.get('wandb-api-key')\n","\n","  root_dir = '/content/images256'\n","\n","else:\n","  from kaggle_secrets import UserSecretsClient\n","  user_secrets = UserSecretsClient()\n","  wandb_api_key = user_secrets.get_secret(\"wandb-api-key\")\n","\n","  root_dir = '/kaggle/input'\n","\n","import wandb\n","wandb.login(key=wandb_api_key)"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["batch_size = 32\n","n_epochs = 500\n","learning_rate = 1e-3\n","dropout = 0.2\n","train_size = 0.8\n","limit_per_class = 0  # 0 to disable\n","seed = 42\n","save_every = 1\n","n_patches = 32\n","n_embedding = 128"]},{"cell_type":"markdown","metadata":{},"source":["# WandB"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_name = \"ViT-17classes\"\n","\n","wandb.init(\n","    project=\"deep-learning\",\n","    config={\n","        \"model\": model_name,\n","        \"batch_size\": batch_size,\n","        \"n_epochs\": n_epochs,\n","        \"n_patches\": n_patches,\n","        \"n_embedding\": n_embedding,\n","        \"learning_rate\": learning_rate,\n","        \"dropout\": dropout,\n","        \"train_size\": train_size,\n","        \"seed\": seed,\n","        \"limit_per_class\": limit_per_class,\n","    }\n",")"]},{"cell_type":"markdown","metadata":{"id":"xZos33KvJZyI"},"source":["# PlacesDataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class PlacesDataset(data.Dataset):\n","    def __init__(self, root_dir=root_dir, train=True, train_size=train_size, seed=seed, limit_per_class=limit_per_class):\n","        super().__init__()\n","\n","        # params dataset\n","        self.root_dir = root_dir\n","        self.train = train\n","        self.limit_per_class = limit_per_class\n","\n","        # data and labels\n","        self.image_paths = []\n","        self.labels = []\n","\n","        self.number_of_classes = 0\n","        self.classes = []\n","        self.old_labels = []\n","\n","        self.transform = transforms.Compose([\n","            transforms.Resize((256, 256)),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomVerticalFlip(),\n","            transforms.RandomRotation(degrees=30), # degrees = range of rotation\n","            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # parameters are ranges\n","            transforms.RandomGrayscale(p=0.1), # p = probability of applying the transform\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","\n","        list_sub_catg = ['shop', 'outdoor', 'outdoor', 'cultivated', 'wild',\n","                         'outdoor', 'coral_reef', 'outdoor', 'indoor', 'outdoor',\n","                         'outdoor', 'indoor', 'football', 'baseball', 'platform',\n","                         'outdoor', 'outdoor', 'platform', 'south_asia', 'east_asia',\n","                         'outdoor', 'outdoor', 'home', 'outdoor', 'sand',\n","                         'vegetation']\n","\n","        self.class_mapping = {\n","            # SKIP\n","            'ballroom': 'skip', 'courtyard': 'skip', 'fire_escape': 'skip', 'garbage_dump': 'skip', 'hospital': 'skip', 'hot_spring': 'skip', 'inn': 'skip',\n","            'phone_booth': 'skip', 'picnic_area': 'skip', 'reception': 'skip', 'restaurant_kitchen': 'skip', 'sky': 'skip', 'tree_farm': 'skip', 'trench': 'skip',\n","            'valley': 'skip', 'water_tower': 'skip', 'wind_farm': 'skip', 'windmill': 'skip',\n","\n","            # highly frequented public\n","            'airport_terminal': 'high_freq_pub', 'amusement_park': 'high_freq_pub', 'bar': 'high_freq_pub', 'food_court': 'high_freq_pub', 'cafeteria': 'high_freq_pub', 'playground': 'high_freq_pub', 'plaza': 'high_freq_pub',\n","            'pulpit': 'high_freq_pub', 'restaurant': 'high_freq_pub', 'restaurant_patio': 'high_freq_pub', 'stage': 'high_freq_pub', 'subway_station': 'high_freq_pub', 'train_station': 'high_freq_pub', 'platform': 'high_freq_pub',\n","\n","            # highly frequented not public\n","            'auditorium': 'high_freq_n_pub', 'banquet_hall': 'high_freq_n_pub', 'conference_center': 'high_freq_n_pub', 'classroom': 'high_freq_n_pub', 'kindergarden_classroom': 'high_freq_n_pub', 'lobby': 'high_freq_n_pub',\n","            'locker_room': 'high_freq_n_pub', 'office': 'high_freq_n_pub', 'television_studio': 'high_freq_n_pub',\n","\n","            # monument\n","            'basilica': 'monument', 'bridge': 'monument', 'castle': 'monument', 'cathedral': 'monument', 'abbey': 'monument', 'amphitheater': 'monument', 'aqueduct': 'monument', 'arch': 'monument',\n","            'cemetery': 'monument', 'church': 'monument', 'dam': 'monument', 'kasbah': 'monument', 'lighthouse': 'monument', 'mausoleum': 'monument', 'monastery': 'monument', 'pagoda': 'monument',\n","            'palace': 'monument', 'pavilion': 'monument', 'ruin': 'monument', 'temple': 'monument', 'tower': 'monument', 'viaduct': 'monument',\n","\n","            # urban\n","            'apartment_building': 'urban', 'building_facade': 'urban', 'gas_station': 'urban', 'medina': 'urban', 'office_building': 'urban', 'skyscraper': 'urban',\n","            'slum': 'urban',\n","\n","            # aquatic\n","            'aquarium': 'aquatic', 'ocean': 'aquatic', 'raft': 'aquatic', 'river': 'aquatic', 'underwater': 'aquatic',\n","\n","            # near water\n","            'bayou': 'near_water', 'coast': 'near_water', 'creek': 'near_water', 'dock': 'near_water', 'fountain': 'near_water', 'harbor': 'near_water', 'iceberg': 'near_water', 'islet': 'near_water', 'marsh': 'near_water',\n","            'pond': 'near_water', 'sandbar': 'near_water', 'swamp': 'near_water', 'swimming_pool': 'near_water', 'watering_hole': 'near_water',\n","\n","            # commercial\n","            'art_gallery': 'commercial', 'art_studio': 'commercial', 'bakery': 'commercial', 'beauty_salon': 'commercial', 'bookstore': 'commercial', 'bowling_alley': 'commercial',\n","            'butchers_shop': 'commercial', 'candy_store': 'commercial', 'clothing_store': 'commercial', 'coffee_shop': 'commercial', 'gift_shop': 'commercial',\n","            'ice_cream_parlor': 'commercial', 'laundromat': 'commercial', 'market': 'commercial', 'museum': 'commercial', 'shoe_shop': 'commercial', 'shopfront': 'commercial', 'supermarket': 'commercial',\n","\n","            # industrial\n","            'assembly_line': 'industrial', 'construction_site': 'industrial', 'engine_room': 'industrial', 'excavation': 'industrial', 'fire_station': 'industrial',\n","\n","            # domestic\n","            'attic': 'domestic', 'basement': 'domestic', 'bedroom': 'domestic', 'closet': 'domestic', 'conference_room': 'domestic', 'corridor': 'domestic', 'dinette': 'domestic',\n","            'dining_room': 'domestic', 'dorm_room': 'domestic', 'galley': 'domestic', 'game_room': 'domestic', 'home_office': 'domestic', 'hospital_room': 'domestic', 'hotel_room': 'domestic',\n","            'jail_cell': 'domestic', 'kitchen': 'domestic', 'kitchenette': 'domestic', 'living_room': 'domestic', 'music_studio': 'domestic', 'nursery': 'domestic', 'pantry': 'domestic',\n","            'parlor': 'domestic', 'shower': 'domestic', 'staircase': 'domestic', 'veranda': 'domestic', 'waiting_room': 'domestic',\n","\n","            # natural vegetal\n","            'bamboo_forest': 'nat_vege', 'campsite': 'nat_vege', 'forest_path': 'nat_vege', 'rainforest': 'nat_vege', 'rope_bridge': 'nat_vege',\n","\n","            # natural mineral\n","            'badlands': 'nat_miner', 'butte': 'nat_miner', 'canyon': 'nat_miner', 'crevasse': 'nat_miner', 'desert': 'nat_miner', 'mountain': 'nat_miner', 'mountain_snowy': 'nat_miner',\n","            'rock_arch': 'nat_miner', 'sea_cliff': 'nat_miner', 'snowfield': 'nat_miner', 'volcano': 'nat_miner',\n","\n","            # domesticated nature\n","            'botanical_garden': 'domest_nat', 'corn_field': 'domest_nat', 'cottage_garden': 'domest_nat', 'fairway': 'domest_nat', 'field': 'domest_nat', 'formal_garden': 'domest_nat',\n","            'golf_course': 'domest_nat', 'herb_garden': 'domest_nat', 'orchard': 'domest_nat', 'pasture': 'domest_nat', 'rice_paddy': 'domest_nat', 'shed': 'domest_nat',\n","            'topiary_garden': 'domest_nat', 'vegetable_garden': 'domest_nat', 'wheat_field': 'domest_nat', 'yard': 'domest_nat',\n","\n","            # sport\n","            'baseball_field': 'sport', 'boxing_ring': 'sport', 'ice_skating_rink': 'sport', 'martial_arts_gym': 'sport', 'racecourse': 'sport',\n","            'ski_resort': 'sport', 'ski_slope': 'sport', 'stadium': 'sport', 'track': 'sport',\n","\n","            # transport\n","            'boat_deck': 'transport', 'bus_interior': 'transport', 'cockpit': 'transport',\n","\n","            # homes\n","            'chalet': 'home', 'courthouse': 'home', 'doorway': 'home', 'driveway': 'home', 'hotel': 'home', 'igloo': 'home', 'mansion': 'home', 'motel': 'home',\n","            'patio': 'home', 'residential_neighborhood': 'home', 'schoolhouse': 'home',\n","\n","            # roads\n","            'forest_road': 'roads', 'alley': 'roads', 'boardwalk': 'roads', 'crosswalk': 'roads', 'highway': 'roads', 'parking_lot': 'roads',\n","            'railroad_track': 'roads', 'runway': 'roads', 'train_railway': 'roads'\n","        }\n","\n","        keys = ['domest_nat', 'home', 'monument', 'transport', 'near_water', 'high_freq_n_pub', 'aquatic', 'commercial',\n","                'industrial', 'domestic', 'high_freq_pub', 'nat_vege', 'sport', 'nat_miner', 'roads', 'urban', 'skip']\n","        values = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n","\n","        self.nb_mapping = {k: v for k, v in zip(keys, values)}\n","\n","        self.number_of_classes = len(keys) - 1\n","\n","        class_idx = 0\n","        for letter_folder in os.listdir(root_dir):\n","            letter_folder_path = os.path.join(root_dir, letter_folder)\n","            if not os.path.isdir(letter_folder_path):\n","                continue  # Skip if not a directory\n","\n","            # Iterate through class folders within the alphabetical folder\n","            for class_name in os.listdir(letter_folder_path):\n","                class_dir = os.path.join(letter_folder_path, class_name)\n","                if not os.path.isdir(class_dir):\n","                    continue  # Skip if not a directory\n","\n","                filenames = sorted(os.listdir(class_dir))\n","                count = 0\n","                for filename in filenames:\n","                    if limit_per_class == 0 or count < limit_per_class:\n","                      # sub category is detected\n","                      if filename in list_sub_catg:\n","                        print(\n","                            f\"Sub-category detected '{filename}' and images label with parent category {class_name} {class_idx}\")\n","\n","                        sub_class_dir = os.path.join(class_dir, filename)\n","                        sub_filenames = sorted(os.listdir(sub_class_dir))\n","\n","                        for sub_filename in sub_filenames:\n","                          if limit_per_class == 0 or count < limit_per_class:\n","                              try:\n","\n","                                  label_map = self.class_mapping[class_name]\n","                                  if label_map != 'skip':\n","                                    sub_im_path = os.path.join(\n","                                        sub_class_dir, sub_filename)\n","\n","                                    Image.open(sub_im_path).verify()\n","\n","                                    self.image_paths.append(sub_im_path)\n","\n","                                    self.old_labels.append(class_name)\n","\n","                                    label_idx = self.nb_mapping[label_map]\n","\n","                                    self.labels.append(label_idx)\n","\n","                                    count += 1\n","                                  else:\n","                                    print(f\"skip class {class_name}\")\n","                                    break\n","\n","                              except (IOError, SyntaxError):\n","                                  print(\n","                                      'SUB Corrupted image or non-image file detected and skipped:', sub_filename)\n","                      else:\n","                        try:\n","                            label_map = self.class_mapping[class_name]\n","                            if label_map != 'skip':\n","                                im_path = os.path.join(class_dir, filename)\n","\n","                                Image.open(im_path).verify()\n","\n","                                self.image_paths.append(im_path)\n","\n","                                self.old_labels.append(class_name)\n","\n","                                label_map = self.class_mapping[class_name]\n","\n","                                label_idx = self.nb_mapping[label_map]\n","\n","                                self.labels.append(label_idx)\n","\n","                                count += 1\n","                            else:\n","                                print(f\"skip class {class_name}\")\n","                                break\n","\n","                        except (IOError, SyntaxError):\n","                          print(\n","                              'Corrupted image or non-image file detected and skipped:', filename)\n","                    else:\n","                        break\n","                self.classes.append(class_name)\n","                class_idx += 1\n","\n","        combined = list(zip(self.image_paths, self.labels))\n","\n","        random.seed(seed)\n","        random.shuffle(combined)\n","\n","        self.image_paths, self.labels = zip(*combined)\n","\n","        nb_train = int(train_size * len(self.image_paths))\n","\n","        # Dividing the dataset\n","        if self.train:\n","          self.image_paths = self.image_paths[:nb_train]\n","          self.labels = self.labels[:nb_train]\n","        else:\n","          self.image_paths = self.image_paths[nb_train:]\n","          self.labels = self.labels[nb_train:]\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, index):\n","        image_path = self.image_paths[index]\n","        label = self.labels[index]\n","\n","        try:\n","          image = Image.open(image_path)\n","          op = 1\n","          image = image.convert('RGB')\n","          rgb = 1\n","        except (IOError, SyntaxError):\n","          print(f\"Error convert to load {op} RGB {rgb} : {image_path} {label}\")\n","\n","        image = self.transform(image)\n","\n","        return image, label"]},{"cell_type":"markdown","metadata":{"id":"rW6Ju2wtJjQh"},"source":["# ViT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djE8-SIgIpGq","trusted":true},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = 'cuda'\n","else :\n","    device = 'cpu'\n","\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DfCk4fZMIpGr","trusted":true},"outputs":[],"source":["def patchify(images, n_patches):\n","    n, c, h, w = images.shape\n","\n","    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n","    patch_size = h // n_patches\n","\n","    for idx, image in enumerate(images):\n","        for i in range(n_patches):\n","            for j in range(n_patches):\n","                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n","                patches[idx, i * n_patches + j] = patch.flatten()\n","    return patches"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, n_heads, head_size, n_patches, n_embedding):\n","        super(MultiHeadAttention, self).__init__()\n","        self.n_heads = n_heads\n","        self.head_size = head_size\n","        \n","        self.q_mappings = nn.ModuleList([nn.Linear(self.head_size, self.head_size) for _ in range(self.n_heads)])\n","        self.k_mappings = nn.ModuleList([nn.Linear(self.head_size, self.head_size) for _ in range(self.n_heads)])\n","        self.v_mappings = nn.ModuleList([nn.Linear(self.head_size, self.head_size) for _ in range(self.n_heads)])\n","        self.head_dropout = nn.ModuleList([nn.Dropout(dropout) for _ in range(self.n_heads)])\n","\n","        self.softmax = nn.Softmax(dim=-1)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, sequences):\n","        # B : batch size\n","        # L : length of the sequence\n","        # D : dimension of each token\n","        # Sequences has shape (B, L, D)\n","        \n","        # N : number of heads\n","        # S : size of each head \n","        \n","        result = []\n","        for sequence in sequences:\n","            seq_result = []\n","            for head in range(self.n_heads):\n","                q_mapping = self.q_mappings[head]\n","                k_mapping = self.k_mappings[head]\n","                v_mapping = self.v_mappings[head]\n","                \n","                #each head operates on a given segment of the sequence\n","                seq = sequence[:, head * self.head_size: (head + 1) * self.head_size] #slices of shape (L, D_h)\n","                \n","                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq) #slices of shape (L, D_h)\n","\n","                attention = self.softmax(q @ k.T / (self.head_size ** 0.5)) # (L, D_h) x (D_h, L) --> (L, L)\n","                dropout_layer = self.head_dropout[head]\n","                attention = dropout_layer(attention)  #(L, L)\n","                \n","                seq_result.append(attention @ v) #(L, L) x (L, D_h) --> (L, D_h)\n","                \n","            #concatenate \n","            result.append(torch.hstack(seq_result)) # concatenation of N slice of shape (L, D_h) --> (L, D)\n","            \n","        h = torch.cat([torch.unsqueeze(r, dim=0) for r in result])#stack all sequences along a new dimesions : back to (B, L, D)\n","        return h"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class FeedForward(nn.Module):\n","    def __init__(self, n_embedding):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embedding, 4*n_embedding),\n","            nn.GELU(),\n","            nn.Linear(4*n_embedding, n_embedding),\n","            nn.Dropout(dropout)\n","        )\n","    \n","    def forward(self, x):\n","        return self.net(x)  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Block(nn.Module):\n","    def __init__(self, n_embedding, n_heads, n_patches):\n","        super(Block, self).__init__()\n","        self.n_embedding = n_embedding\n","        self.n_heads = n_heads\n","        self.n_patches = n_patches\n","\n","        head_size = self.n_embedding // n_heads\n","\n","        self.sa = MultiHeadAttention(self.n_heads, head_size, self.n_patches, self.n_embedding)\n","        self.ff = FeedForward(self.n_embedding)\n","        self.ln1 = nn.LayerNorm(self.n_embedding)\n","        self.ln2 = nn.LayerNorm(self.n_embedding)\n","\n","\n","    def forward(self, x):\n","        h = x\n","        h = h + self.sa(self.ln1(h))\n","        h = h + self.ff(self.ln2(h))\n","        return h"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class VisualTransformer(nn.Module):\n","    def __init__(self, dimensions, n_patches, n_embedding, n_heads, output_size):\n","        # Super constructor\n","        super(VisualTransformer, self).__init__()\n","        \n","        # Attributes\n","        self.dimensions = dimensions \n","        self.n_patches = n_patches\n","        self.n_heads = n_heads\n","        self.n_embedding = n_embedding\n","        self.output_size = output_size\n","        \n","        self.patch_size = (dimensions[1] / n_patches, dimensions[2] / n_patches)\n","\n","        # 1) Linear mapper\n","        self.input_d = int(dimensions[0] * self.patch_size[0] * self.patch_size[1])\n","        self.linear_mapper = nn.Linear(self.input_d, self.n_embedding)\n","        \n","        # 2) Learnable classification token\n","        self.class_token = nn.Parameter(torch.rand(1, self.n_embedding))\n","        \n","        # 3) Positional embedding\n","        self.positional_embeddings = nn.Embedding(self.n_patches**2 + 1, self.n_embedding)\n","        \n","        # 4) Transformer encoder blocks\n","        self.blocks = nn.Sequential(\n","            Block(self.n_embedding, self.n_heads, self.n_patches),\n","            Block(self.n_embedding, self.n_heads, self.n_patches),\n","            Block(self.n_embedding, self.n_heads, self.n_patches),\n","            nn.LayerNorm(self.n_embedding)\n","        )\n","        \n","        # 5) Classification MLP\n","        self.lm_head = nn.Sequential(\n","            nn.Linear(self.n_embedding, self.output_size),\n","            nn.Softmax(dim=-1)\n","        )\n","\n","    def forward(self, images):\n","        # Patchify\n","        patches = patchify(images, self.n_patches).to(device)\n","        \n","        #Token Embedding\n","        tok_emb = self.linear_mapper(patches)\n","        \n","        # Adding classification token\n","        tok_emb = torch.stack([torch.vstack((self.class_token, tok_emb[i]))for i in range(len(tok_emb))])\n","        \n","        #Positional Embedding\n","        pos_emb = self.positional_embeddings(torch.arange(self.n_patches**2 + 1, device=device))\n","        pos_emb = pos_emb.unsqueeze(0)\n","        pos_emb = pos_emb.expand(tok_emb.shape[0], -1, -1)  # Expand along the batch dimension\n","        \n","        #Sum Token and Positional Embedding\n","        h = tok_emb + pos_emb\n","        \n","        # Transformer Blocks\n","        h = self.blocks(h)\n","            \n","        # Getting the classification token only\n","        h = h[:, 0]\n","        \n","        #Final head\n","        h = self.lm_head(h)\n","        \n","        return h"]},{"cell_type":"markdown","metadata":{"id":"Vu505RVsJm1L"},"source":["# Train loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zs4Fk7CIpGs","trusted":true},"outputs":[],"source":["def train(model, trainloader, testloader, n_epochs=n_epochs, learning_rate=learning_rate):\n","\n","    model.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    train_avg_loss = []\n","    test_avg_loss = []\n","    test_accuracy = []\n","\n","    for i in range(n_epochs):\n","\n","        print(f\"Epoch : {i}\")\n","\n","        train_losses = []\n","        test_losses = []\n","        \n","        # train\n","        for x, y in trainloader:\n","            # send to device\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            # predict\n","            pred = model(x)\n","            loss = criterion(pred, y)\n","            train_losses.append(loss.detach())\n","\n","            # step\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        # test\n","        with torch.no_grad():\n","            correct = 0\n","\n","            for x,y in testloader:\n","                x = x.to(device)\n","                y = y.to(device)\n","\n","                pred = model(x)\n","                loss = criterion(pred, y)\n","                test_losses.append(loss.detach())\n","\n","                y_pred = pred.argmax(dim=-1)\n","                correct = correct + (y_pred==y).sum()\n","\n","            accuracy = (correct / len(testloader.dataset))\n","\n","        train_loss = torch.stack(train_losses).mean()\n","        test_loss = torch.stack(test_losses).mean()\n","\n","        print(f\"train_losses : {train_loss}\")\n","        print(f\"test_losses : {test_loss}\")\n","        print(f\"accuracy : {accuracy}\")\n","        \n","        wandb.log({\n","            \"epoch\": i,\n","            \"train loss\": train_loss,\n","            \"test loss\": test_loss,\n","            \"accuracy\": accuracy,\n","        })\n","        \n","        if i % save_every == 0:\n","            torch.save(model.state_dict(), f\"epoch_{i}_model.pt\")\n","            wandb.save(f\"epoch_{i}_model.pt\")\n","\n","        train_avg_loss.append(train_loss)\n","        test_avg_loss.append(test_loss)\n","        test_accuracy.append(accuracy)\n","\n","    return train_avg_loss, test_avg_loss, test_accuracy"]},{"cell_type":"markdown","metadata":{"id":"6bhhihxEJyKI"},"source":["# Create dataset / dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldg8clQWjX18","trusted":true},"outputs":[],"source":["# Instantiate the train and test set\n","\n","# train\n","train_dataset = PlacesDataset(train=True)\n","\n","# test\n","test_dataset = PlacesDataset(train=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htjtuyMkIpGt","trusted":true},"outputs":[],"source":["# Instantiate the corresponding data loaders\n","\n","# train\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","# test\n","test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"Lwt1N-OEJ4GM"},"source":["# Define Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0F8G1C9VIpGu","trusted":true},"outputs":[],"source":["input_features = [3, 256, 256] # Channels (assuming RGB images), Height, Width\n","output_features = train_dataset.number_of_classes\n","\n","network = VisualTransformer(input_features, n_patches, n_embedding, 4, output_features).to(device)\n","print(\"Parameters:\", sum(p.numel() for p in network.parameters())/1e3, 'K parameters')\n","print(network)"]},{"cell_type":"markdown","metadata":{"id":"4x86umi-KF2r"},"source":["# Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Qvj8tuPOIpGu","trusted":true},"outputs":[],"source":["train_avg_loss, test_avg_loss, test_accuracy = train(model=network,\n","                                                     trainloader=train_loader,\n","                                                     testloader=test_loader,\n","                                                     n_epochs=n_epochs,\n","                                                     learning_rate=learning_rate\n","                                                     )"]},{"cell_type":"markdown","metadata":{"id":"34GemFNqKO6q"},"source":["# Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fp6bD0W-IpGv","trusted":true},"outputs":[],"source":["train_avg_loss_np = torch.tensor(train_avg_loss).detach().cpu().numpy()\n","test_avg_loss_np = torch.tensor(test_avg_loss).detach().cpu().numpy()\n","test_accuracy_np = torch.tensor(test_accuracy).detach().cpu().numpy()\n","\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(train_avg_loss_np, label='Training Loss')\n","plt.plot(test_avg_loss_np, label='Testing Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Testing Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(test_accuracy_np, label='Test Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Test Accuracy')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Finish wandb run"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# necessary in notebooks\n","wandb.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":263563,"sourceId":559659,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
